{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def call_llm_agent(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder for your LLM agent function that processes the prompt and returns a description.\n",
    "    Replace this with your actual LLM agent implementation.\n",
    "    \"\"\"\n",
    "    # This is a mock implementation - replace with your actual LLM agent call\n",
    "    table_name = prompt.split(\"Table: \")[1].split(\",\")[0]\n",
    "    \n",
    "    # Mock table descriptions based on common database patterns\n",
    "    # You can enhance this with your actual LLM agent\n",
    "    return f\"This table stores and manages {table_name} related data with associated metadata and relationships.\"\n",
    "\n",
    "def generate_table_description(table_name: str, columns: List[Dict], sample_data: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Generate a single description for the entire table using LLM agent.\n",
    "    \"\"\"\n",
    "    # Create column info string for context\n",
    "    column_info = []\n",
    "    for column in columns:\n",
    "        column_name = list(column.keys())[0]\n",
    "        column_type = column[column_name]\n",
    "        column_info.append(f\"{column_name} ({column_type})\")\n",
    "    \n",
    "    columns_str = \", \".join(column_info)\n",
    "    \n",
    "    # Create sample data string for context\n",
    "    sample_titles = []\n",
    "    for sample in sample_data[:3]:  # Use first 3 samples for context\n",
    "        # Try to find a descriptive field (title, name, etc.)\n",
    "        for field in ['title', 'name', 'description', 'label']:\n",
    "            if field in sample and sample[field]:\n",
    "                sample_titles.append(str(sample[field]))\n",
    "                break\n",
    "    \n",
    "    sample_str = \", \".join(sample_titles) if sample_titles else \"No sample data available\"\n",
    "    \n",
    "    prompt = f\"Table: {table_name}, Columns: {columns_str}, Sample Data: {sample_str}\"\n",
    "    description = call_llm_agent(prompt)\n",
    "    \n",
    "    return description\n",
    "\n",
    "def read_schema_from_file(file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Read schema from a Python file that contains 'schema = {...}'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Find the schema definition using regex\n",
    "        schema_match = re.search(r'schema\\s*=\\s*({.*?})\\s*(?=\\n\\S|\\n*$)', content, re.DOTALL)\n",
    "        \n",
    "        if schema_match:\n",
    "            schema_str = schema_match.group(1)\n",
    "            # Use eval to parse the schema (be careful with this in production)\n",
    "            # For safety, you might want to use ast.literal_eval if the schema is simpler\n",
    "            schema = eval(schema_str)\n",
    "            return schema\n",
    "        else:\n",
    "            print(f\"Could not find schema definition in {file_path}\")\n",
    "            return {}\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading schema from {file_path}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def write_schema_to_file(file_path: str, schema: Dict):\n",
    "    \"\"\"\n",
    "    Write the updated schema back to the Python file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the original file content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Convert schema to a properly formatted string\n",
    "        schema_str = format_schema_for_file(schema)\n",
    "        \n",
    "        # Replace the schema definition\n",
    "        updated_content = re.sub(\n",
    "            r'schema\\s*=\\s*{.*?}',\n",
    "            f'schema = {schema_str}',\n",
    "            content,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        \n",
    "        # Write back to file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(updated_content)\n",
    "            \n",
    "        print(f\"Updated schema saved to {file_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing schema to {file_path}: {str(e)}\")\n",
    "\n",
    "def format_schema_for_file(schema: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Format schema dictionary as a properly indented Python dictionary string\n",
    "    \"\"\"\n",
    "    def format_value(value, indent=0):\n",
    "        spaces = \"    \" * indent\n",
    "        if isinstance(value, dict):\n",
    "            if not value:\n",
    "                return \"{}\"\n",
    "            lines = [\"{\"]\n",
    "            for k, v in value.items():\n",
    "                formatted_v = format_value(v, indent + 1)\n",
    "                if isinstance(v, str):\n",
    "                    lines.append(f'{spaces}    \"{k}\": \"{formatted_v}\",')\n",
    "                else:\n",
    "                    lines.append(f'{spaces}    \"{k}\": {formatted_v},')\n",
    "            lines.append(f\"{spaces}}}\")\n",
    "            return \"\\n\".join(lines)\n",
    "        elif isinstance(value, list):\n",
    "            if not value:\n",
    "                return \"[]\"\n",
    "            lines = [\"[\"]\n",
    "            for item in value:\n",
    "                formatted_item = format_value(item, indent + 1)\n",
    "                lines.append(f\"{spaces}    {formatted_item},\")\n",
    "            lines.append(f\"{spaces}]\")\n",
    "            return \"\\n\".join(lines)\n",
    "        elif isinstance(value, str):\n",
    "            return value\n",
    "        else:\n",
    "            return repr(value)\n",
    "    \n",
    "    return format_value(schema)\n",
    "\n",
    "def update_schema_with_descriptions(schema: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Update the schema dictionary to include a single description for each table.\n",
    "    \"\"\"\n",
    "    updated_schema = {}\n",
    "    \n",
    "    for table_name, table_info in schema.items():\n",
    "        print(f\"Processing table: {table_name}\")\n",
    "        \n",
    "        # Generate description for this table\n",
    "        table_description = generate_table_description(\n",
    "            table_name, \n",
    "            table_info[\"columns\"], \n",
    "            table_info[\"sample\"]\n",
    "        )\n",
    "        \n",
    "        # Create updated table structure with original column format\n",
    "        columns = {}\n",
    "        for column in table_info[\"columns\"]:\n",
    "            column_name = list(column.keys())[0]\n",
    "            column_type = column[column_name]\n",
    "            columns[column_name] = column_type\n",
    "        \n",
    "        updated_schema[table_name] = {\n",
    "            \"columns\": columns,\n",
    "            \"sample\": table_info[\"sample\"],\n",
    "            \"description\": table_description\n",
    "        }\n",
    "    \n",
    "    return updated_schema\n",
    "\n",
    "def process_database_files():\n",
    "    \"\"\"\n",
    "    Process all database schema files in the current directory\n",
    "    \"\"\"\n",
    "    # List of database files to process\n",
    "    db_files = [\"chinook_db.py\", \"netflix_db.py\"]\n",
    "    \n",
    "    for db_file in db_files:\n",
    "        if os.path.exists(db_file):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing {db_file}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Read schema from file\n",
    "            schema = read_schema_from_file(db_file)\n",
    "            \n",
    "            if schema:\n",
    "                # Update schema with descriptions\n",
    "                updated_schema = update_schema_with_descriptions(schema)\n",
    "                \n",
    "                # Write back to file\n",
    "                write_schema_to_file(db_file, updated_schema)\n",
    "                \n",
    "                print(f\"\\nCompleted processing {db_file}\")\n",
    "                print(f\"Tables processed: {list(updated_schema.keys())}\")\n",
    "            else:\n",
    "                print(f\"No schema found in {db_file}\")\n",
    "        else:\n",
    "            print(f\"File {db_file} not found in current directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all database files\n",
    "    process_database_files()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"All database schema files have been processed!\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
